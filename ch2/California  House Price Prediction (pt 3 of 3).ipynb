{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <i>Hands on Machine Learning with Scikit-Learn and Tensorflow</i>\n",
    "\n",
    "\n",
    "# Chapter 2 - California House Price Prediction \n",
    "\n",
    "(Part 3 of 3)\n",
    "\n",
    "The objective of this model is to predict house prices given a California census data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# housekeeping stuff to set the page width as wide as possible in this notebook\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import the data from part 2\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Need to copy and paste some classes from part 2. There's surely a better way to do this.\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].values\n",
    "rooms_ix, bedrooms_ix, population_ix, household_ix = 3, 4, 5, 6\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room = True):        # avoid *args and **kargs\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self      # nothin else to do \n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, household_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, household_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n",
    "# Done with the copy/paste\n",
    "\n",
    "def load_data():\n",
    "    if not os.path.isdir('./data') or not os.path.exists('./data/prt2.pkle'):\n",
    "        raise Exception('No data from part 2 found. Please run part 2 first.')\n",
    "        os.exit(1)\n",
    "\n",
    "    with open('./data/prt2.pkle', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        \n",
    "    return data['housing'], data['housing_prepared'], data['housing_labels'], data['full_pipeline'], data['encoder'], data['num_attribs']\n",
    "\n",
    "housing, housing_prepared, housing_labels, full_pipeline, encoder, num_attribs = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select and Train a Model\n",
    "\n",
    "Steps so far:\n",
    "\n",
    "1. Got raw data and explored it\n",
    "2. Sampled a training and test set\n",
    "3. Wrote transformation pipelines to clean and prepare the data\n",
    "4. Finally it's time for Machine Learning Algorithms\n",
    "\n",
    "Let's start by training a <b>Linear Regression</b> model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done!! You now have a working Linear Regression model. Let's try it out on a few instances from the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\t [210644, 317768, 210956, 59218, 189747]\n"
     ]
    }
   ],
   "source": [
    "some_data = housing.iloc[:5]\n",
    "some_labels = housing_labels.iloc[:5]\n",
    "some_data_prepared = full_pipeline.transform(some_data)\n",
    "\n",
    "print(\"Predictions:\\t\", list(lin_reg.predict(some_data_prepared).astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:\t\t [286600, 340600, 196900, 46300, 254500]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Labels:\\t\\t\", list(some_labels.astype(int)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works! But we can do better. Let's measure the Root Mean Squared Error (RMSE) on the whole training set using Scikit-Learn's <code>mean_squared_error()</code> function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68628.198198489234"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "housing_predictions = lin_reg.predict(housing_prepared)\n",
    "lin_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "lin_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better than nothing, but not great... most districts median_housing_values range between 120,000 USD and 265,000 USD so a typical prediction error of $68,628 is not very satisfying.\n",
    "\n",
    "<i>The model is underfitting the data.</i> When this happens it can mean that the features do not provide enough information to make good predictions or that the model is not powerful enough. \n",
    "\n",
    "<B> Possible solutions for underfitting</b>:\n",
    "\n",
    "1. select a more powerful model\n",
    "2. feed the training algorithm better features\n",
    "3. reduce the contraints on the model\n",
    "\n",
    "The model is not regularized so option 3 is not available. You could try to add more features (e.g. the log of the population) but first let's try a more complex model.\n",
    "\n",
    "Let's traing a <b><code>DecisionTreeRegressor</code></b> (covered in Chapter 6). This is a powerful model capable of finding complex nonlinear relationships in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "tree_reg.fit(housing_prepared, housing_labels)\n",
    "\n",
    "housing_predictions = tree_reg.predict(housing_prepared)\n",
    "tree_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "tree_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh shit! Zero error is not a good sign. This model <b>has now badly overfit the data</b>. How can you be sure? As we saw earlier you don't want to touch the test set until you are ready to launch a model you are confident in so you need to use part of the training set for training and part for model validation.\n",
    "\n",
    "## Better Evaluation Using Cross-Validation\n",
    "\n",
    "To evaluate the Decision Tree you could use the <code>train_test_split()</code> method to split the training set into a smaller training set and a validation set, then train your models against the smaller training set and evaluate them against the validation set. Would work fine but there is a better alternative. \n",
    "\n",
    "Use Scikit-Learn's <i>cross-validation</i> feature. The following code performs <b><i>K-Fold cross-validation</i></b>: it randomly splits the training set into 10 distinct subsets called <i>folds</i>, then it trains and evaluates the Decision Tree model 10 times, picking a different fold for evaluation each time and training on the other 9 folds. The result is an array containing the 10 evaluation scores:\n",
    "\n",
    "<i>(NOTE: sklearn Cross-Validation expects a utility function (greater is better) rather than a cost function (lower is better) so the scoring function is actually the opposite of the MSE (i.e. a negative value) which is why the code computes -scores befire calculating the square root)</i>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores\t\t\t: [68654 67610 70981 70563 71847 75512 70146 70308 78232 70341]\n",
      "Mean\t\t\t: 71419\n",
      "Standard Deviation\t: 3007\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(tree_reg, housing_prepared, housing_labels, \n",
    "                        scoring='neg_mean_squared_error', cv=10)\n",
    "tree_rmse_scores = np.sqrt(-scores)\n",
    "\n",
    "def display_scores(scores):\n",
    "    print(\"Scores\\t\\t\\t:\", scores.astype(int))\n",
    "    print(\"Mean\\t\\t\\t:\", int(scores.mean()))\n",
    "    print(\"Standard Deviation\\t:\", int(scores.std()))\n",
    "\n",
    "display_scores(tree_rmse_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DecisionTree seems to be worse than the LinearRegression model now!! Notice that the cross-validation allows you to get not only an estimate of the performance of your model but also a measure of how precise this estimate is (i.e. it's Standard Deviation). The Decision Tree has a score of approximately 71,422 +/- 2,696 (re-running the notebook will give different values due to the random selections). You would not have this information if you just used a single validation set. But cross-validation comes at the cost of training the model several times which is not always possible. \n",
    "\n",
    "Let's compute the same scores for the Linear Regression model just to be sure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores\t\t\t: [66782 66960 70347 74739 68031 71193 64969 68281 71552 67665]\n",
      "Mean\t\t\t: 69052\n",
      "Standard Deviation\t: 2731\n"
     ]
    }
   ],
   "source": [
    "lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,\n",
    "                            scoring='neg_mean_squared_error', cv=10)\n",
    "\n",
    "lin_rmse_scores = np.sqrt(-lin_scores)\n",
    "display_scores(lin_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decision Tree model is overfitting the data so badly that it performs worse than the Linear Regression model.\n",
    "\n",
    "Let's try another model: <b><i>RandomForestRegressor</i></b> (detail in Chapter 7). Random Forests work by training many Decision Trees on random subsets of the features, then averaging out their predictions. Building a model on top of many other models is called <i>Ensemble Learning</i> and it is often a great way to push ML algorithms even further:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forest training RMSE\t: 22259\n",
      "Scores\t\t\t: [52051 49709 53495 54645 52892 56637 51670 51302 55740 52555]\n",
      "Mean\t\t\t: 53070\n",
      "Standard Deviation\t: 2005\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest_reg = RandomForestRegressor()\n",
    "\n",
    "forest_reg.fit(housing_prepared, housing_labels)\n",
    "forest_predictions = forest_reg.predict(housing_prepared)\n",
    "forest_mse = mean_squared_error(housing_labels, forest_predictions)\n",
    "forest_rmse = np.sqrt(forest_mse)\n",
    "print('Forest training RMSE\\t:', int(forest_rmse))\n",
    "\n",
    "forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels,\n",
    "                               scoring='neg_mean_squared_error', cv=10)\n",
    "forest_rmse_scores = np.sqrt(-forest_scores)\n",
    "display_scores(forest_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better!! Random Forests look very promising. But note that the score on the training set is still much lower than on the validation sets meaning that the model is still overfitting the training set. \n",
    "\n",
    "<b>Possible solutions for overfitting</b>:\n",
    "\n",
    "1. simplify the model\n",
    "2. constrain it (i.e. regularize it)\n",
    "3. get a lot more training data\n",
    "\n",
    "However, before diving further into Random Forests you should try out many other models (e.g. Support Vector Machines with different kernels, possibly a neural network, etc) without spending too much time tweaking the hyperparameters. The goal is to shortlist 4 to 5 promising models. \n",
    "\n",
    "You should save every model you experiement with so you can come back easily to any model you want. Make sure you also save the hyperparameters and the trained parameters as well as the cross-validation scores and perhaps the actual predictions as well. This will allow you to easily compare scores across model types and compare the types of errors they make. You can save sklearn models using the pickle python module which is efficient at serializing large NumPy arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tune Your Model\n",
    "\n",
    "### Grid Search\n",
    "\n",
    "Instead of playing with hyperameters manually trying to get a great combination, use Scikit-Learn's <code>GridSearchCV</code> to search for you. You need to tell it which parameters you want to experiment with and what values to try out and it will evaluate all the possible combinations of hyperparameter values using cross-validation. Here's an example using for RandomForestRegressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': 6, 'n_estimators': 70}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    {'n_estimators': [10, 50, 60, 70], 'max_features': [2, 4, 6, 8]},\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "]\n",
    "\n",
    "forest_reg = RandomForestRegressor()\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "_ = grid_search.fit(housing_prepared, housing_labels)\n",
    "\n",
    "# Now pint the best combination of parameters\n",
    "print(grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <code>param_grid</code> above tells Scikit-Learn to first evaluate 3x4 = 12 combinactions of n_estimators and max_features (explained in chapter 7) and then try all 2x3 = 6 combinations of hyperameter values in the 2nd dict but this time with the <code>bootstrap</code> hyperparameter set to False (it's default is True).\n",
    "\n",
    "All in all the grid search will explore 12+6 = 18 combinations of RandomForestRegressor <b>and will train each model 5 times since we're using five-fold cross validation</b>. That means 18x5 = 90 rounds of training.\n",
    "\n",
    "<i>Note 1: when you have no idea what values to try for a hyperparmeter, a simple approach is to try out consecuituve powers of 10 (or a small number if you want a more fine graned search).</i><br>\n",
    "\n",
    "You can also get the best estimator directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features=6, max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=70, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the evaluation scores are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55545.5375529 {'max_features': 2, 'n_estimators': 10}\n",
      "52087.4900497 {'max_features': 2, 'n_estimators': 50}\n",
      "52376.9516537 {'max_features': 2, 'n_estimators': 60}\n",
      "52042.3242439 {'max_features': 2, 'n_estimators': 70}\n",
      "53301.3128484 {'max_features': 4, 'n_estimators': 10}\n",
      "50047.9825589 {'max_features': 4, 'n_estimators': 50}\n",
      "49961.5325991 {'max_features': 4, 'n_estimators': 60}\n",
      "49980.8823779 {'max_features': 4, 'n_estimators': 70}\n",
      "52286.9818987 {'max_features': 6, 'n_estimators': 10}\n",
      "49800.2810966 {'max_features': 6, 'n_estimators': 50}\n",
      "49521.0730646 {'max_features': 6, 'n_estimators': 60}\n",
      "49288.5571738 {'max_features': 6, 'n_estimators': 70}\n",
      "52063.3229501 {'max_features': 8, 'n_estimators': 10}\n",
      "49431.888429 {'max_features': 8, 'n_estimators': 50}\n",
      "49466.6150953 {'max_features': 8, 'n_estimators': 60}\n",
      "49563.9499106 {'max_features': 8, 'n_estimators': 70}\n",
      "62112.6941877 {'bootstrap': False, 'max_features': 2, 'n_estimators': 3}\n",
      "54612.6605146 {'bootstrap': False, 'max_features': 2, 'n_estimators': 10}\n",
      "60697.6431042 {'bootstrap': False, 'max_features': 3, 'n_estimators': 3}\n",
      "52760.0683989 {'bootstrap': False, 'max_features': 3, 'n_estimators': 10}\n",
      "58925.2257488 {'bootstrap': False, 'max_features': 4, 'n_estimators': 3}\n",
      "51758.7477564 {'bootstrap': False, 'max_features': 4, 'n_estimators': 10}\n"
     ]
    }
   ],
   "source": [
    "cvres = grid_search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Note: if GridSearchCV is initialized with <code>refit=True</code> (the default) then once it finds the best estimator using cross-validation it retrains it on the whole training set. This is usually a good idea since feeding it more data will likely improve its performance.</i>\n",
    "\n",
    "So the best set up comes out better than the original value from above (~52k USD). \n",
    "\n",
    "Congrats, fine tuning over.\n",
    "\n",
    "<i>Note: don't forget you can treat some of the data preperation steps as hyperparameters. E.g. the grid search will automatically find out whether or not to add a feature you werre not sure about (e.g. the add_bedrooms_per_room hyperparameter of the CombinedAttributesAdder transformer). It may similarly be used to automatically find out the best way to handle outliers, missing features, feature selection, and more.</i>\n",
    "\n",
    "## Randomized Search\n",
    "\n",
    "When the hyperparameter search space is large, it might be preferrable to use <code>RandomizedSearchCV</code> instead. It can be used in much the same way as the GridSearchCV class but instead of trying out all possible combinations, it evaluates a given number of random combinations by selecting a random value for each hyperparameter at every iteration. This approach has two main benefits:\n",
    "\n",
    "1. If you let the randomized search run for say 1000 iterations, this approach will explore 1000 differnt values for each hyperparameter (instead of just the few specified for GridSearchCV)\n",
    "2. You have more control over the computing budget you want to allocate to hyperparameter search, simply by setting the number of iterations. \n",
    "\n",
    "\n",
    "## Ensemble Methods\n",
    "\n",
    "Another way to fine tune your model is to try to combine the models that perform the best. The group (\"ensemble\") will often perform better than the best individual model (just like Random Forests perform better than the single Decision Trees they rely on), especially if the individual models make very different types of errors. This is covered in chapter 7.\n",
    "\n",
    "\n",
    "## Analyze the Best Models and Their Errors\n",
    "\n",
    "You will often get good insights on the problem by inspecting the best modes. For example the RandomForestRegressor can indicate the relative importance of each attribute for making accurate predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  7.67158899e-02   7.37172820e-02   4.22586357e-02   1.82297631e-02\n",
      "   1.73165559e-02   1.78459116e-02   1.64056552e-02   3.23183269e-01\n",
      "   6.38103022e-02   1.06913162e-01   7.74596295e-02   1.28646993e-02\n",
      "   1.44640327e-01   1.00268118e-04   3.41545074e-03   5.12319881e-03]\n"
     ]
    }
   ],
   "source": [
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "print(feature_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display these importance scores nest to their corresponding attribute names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.32318326892113713, 'median_income'),\n",
       " (0.14464032736975097, 'INLAND'),\n",
       " (0.10691316163370912, 'pop_per_hhold'),\n",
       " (0.077459629454888171, 'bedrooms_per_room'),\n",
       " (0.076715889870797097, 'longitude'),\n",
       " (0.073717281968913209, 'latitude'),\n",
       " (0.063810302237230401, 'rooms_per_hhold'),\n",
       " (0.042258635667847412, 'housing_median_age'),\n",
       " (0.018229763119311595, 'total_rooms'),\n",
       " (0.01784591160555906, 'population'),\n",
       " (0.017316555947187576, 'total_bedrooms'),\n",
       " (0.016405655245501027, 'households'),\n",
       " (0.012864699294167265, '<1H OCEAN'),\n",
       " (0.0051231988069661265, 'NEAR OCEAN'),\n",
       " (0.0034154507389063745, 'NEAR BAY'),\n",
       " (0.00010026811812751486, 'ISLAND')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra_attribs = ['rooms_per_hhold', 'pop_per_hhold', 'bedrooms_per_room']\n",
    "cat_one_hot_attribs = list(encoder.classes_)\n",
    "attributes = num_attribs + extra_attribs + cat_one_hot_attribs\n",
    "sorted(zip(feature_importances, attributes), reverse=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this information you might want to try dropping some of the less useful features (e.g. apparently only one ocean_proximity category is really useful, so you could try dropping the others).\n",
    "\n",
    "You should also look at the specific errors that your system makes, then try to understand why it makes them and what could fix the problem (adding extra features or, on the contrary, getting rid of uninformative ones, cleaning up outliers, etc).\n",
    "\n",
    "\n",
    "## Evaluate Your System on the Test Set\n",
    "\n",
    "Now it's time to evaluate the final model on the test set. Just get the predictors and the labels from your test set, run your <code>full_pipeline</code> to transform the data (<b>call <code>transform()</code> and *not* <code>fit_transform()</code> !!!</b>) and evaluate the final model on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46906.0261348\n"
     ]
    }
   ],
   "source": [
    "# First need to go back to part 1 and get the test set\n",
    "with open('./data/prt1.pkle', 'rb') as f:\n",
    "        _, strat_test_set = pickle.load(f)\n",
    "\n",
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "X_test = strat_test_set.drop('median_house_value', axis=1)\n",
    "y_test = strat_test_set['median_house_value'].copy()\n",
    "\n",
    "X_test_prepared = full_pipeline.transform(X_test)\n",
    "\n",
    "final_predictions = final_model.predict(X_test_prepared)\n",
    "\n",
    "final_mse = mean_squared_error(y_test, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse)\n",
    "print(final_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance will ususally be slightly worse than what you measured using the cross-validation if you did a lot of hyperparameter tuning (because your system ends up fine tuned to perform well on the validation data, and will likely not perform as well on unknown datasets). When this happens you must resist the temptation to tweak the hyperparameters to make the numbers look good on the test set: the improvements would be unlikely to generalize to new data.\n",
    "\n",
    "To present this for decision makers, document everything, create presentations with clear visualizations and easy-to-remember statements (e.g. \"the median income is the number one predictor of housing prices\").\n",
    "\n",
    "\n",
    "## Launch, Monitor and Maintain Your System\n",
    "\n",
    "Next you need to get your system ready for production. In particular, plugin the production input data sources and write tests.\n",
    "\n",
    "You also need to write monitoring code to check the live performance at regular intervals and trigger alerts when it drops. This is important to catch not only sudden breakage, but also performance degradataion. This is common because models tend to \"rot\" as data evolves over time unless models are regularly trained on fresh data.\n",
    "\n",
    "Evaluating the perforamce will involve sample the systems predictions and evaluating them. This will generally involve human analysis. These analysts may coworkers, field experts or e.g. Amazon's Mechanical Turk or CrowdFlower. Either way, you need to plug the human evaluation into your system.\n",
    "\n",
    "You should also make sure you evaluate the system's input data quality in case it degrades over time due to a poor quality signal (e.g. a malfunctioning sensor sending random values or another team's output becoming stale) but it may take a while before your montoring triggers an alert. If you monitor the inputs you may catch this earlier. Monitoring the inuts is especially important for online learning systems.\n",
    "\n",
    "Finally you will generally want to train your models on a regular basis using fresh data. You should automate this process as much as possible. If you don't you might e.g. only update your model every 6 months and your system's performance will fuluctuate severly over time. If your system is an online learning system you should make sure you save snapshots of its state at regular intervals so you can easily roll back to a previously working state.\n",
    "\n",
    "\n",
    "# Congratulations!!!! \n",
    "\n",
    "We're done with Chapter 2.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
