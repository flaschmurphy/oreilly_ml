{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <i>Hands on Machine Learning with Scikit-Learn and Tensorflow</i>\n",
    "\n",
    "\n",
    "# Chapter 2 - California House Price Prediction \n",
    "\n",
    "(Part 2 of 3)\n",
    "\n",
    "The objective of this model is to predict house prices given a California census data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# housekeeping stuff to set the page width as wide as possible in this notebook\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Prepare the Data for Machine Learning Algorithms\n",
    "\n",
    "Always write functions for preparing the data:\n",
    "\n",
    "- It will allow you to reproduce these transformations easily on any dataset\n",
    "- You will gradually build up a library of transformation functions \n",
    "- You can use these funtctions in your live system to transform the data before feeding it into your algorithm\n",
    "- It will allow you to easily try different transformations and see what works best\n",
    "\n",
    "First let's revert to a clean dataset and let's separate the predictors and the labels since we don't necessarily want to apply the same transformations to the predictors and the target values.\n",
    "\n",
    "Note: <code>drop()</code> creates a copy of the data and does not affect <code>strat_train_set</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the data from part 1\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "def load_data():\n",
    "    if not os.path.isdir('./data') or not os.path.exists('./data/prt1.pkle'):\n",
    "        raise Exception('No data from part 1 found. Please run part 1 first.')\n",
    "        os.exit(1)\n",
    "\n",
    "    with open('./data/prt1.pkle', 'rb') as f:\n",
    "        strat_train_set, _ = pickle.load(f)\n",
    "        \n",
    "    housing = strat_train_set.drop('median_house_value', axis=1)\n",
    "    housing_labels = strat_train_set['median_house_value'].copy()\n",
    "    \n",
    "    return housing, housing_labels\n",
    "\n",
    "housing, housing_labels = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have two data frames, <code>housing</code> contains everything we can use for predictions, and <code>housing_labels</code> contains the values we are trying to predict.\n",
    "\n",
    "## Data Cleaning\n",
    "\n",
    "Most maching learning algorithms can't work with missing features so let's create a few functions to create them. <code>total_bedrooms</code> from earlier has some missing values and there are 3 options to fix this:\n",
    "\n",
    "- Get rid of the corresponding districts\n",
    "- Get rid of the whole attribute\n",
    "- Set the values to some value (zero, the mean, the median, etc)\n",
    "\n",
    "You can do these easily with <code>DataFrame</code>'s <code>dropna()</code>, <code>drop()</code>, and <code>fillna()</code> methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "housing.dropna(subset=['total_bedrooms'])       # option 1\n",
    "housing.drop('total_bedrooms', axis=1)          # option 2\n",
    "median = housing['total_bedrooms'].median()\n",
    "_ = housing['total_bedrooms'].fillna(median)    # option 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you chose option 3, you should compute the median on the training set and use that value to fill in the missing values in the training set, but don't forget to save the median value you have computed. You will need it later to replace missing values in the test set when you want to evaluate your system, also once the system goes live to replace missing values in new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-Learn Estimators and Transformers\n",
    "\n",
    "Scikit-Learn has a handy class to take care of missing values: <code>Imputer</code>. First you need to create an <code>Imputer</code> instance specifying that you want to replace each attribute's missing values with the median of that attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the data again to be clean as we will use sklearn rather than the 3 options above\n",
    "housing, housing_labels = load_data()\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "imputer = Imputer(strategy='median')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the median can only be computed on numerical attributes, we need to create a copy of the data without the text attribute <code>ocean_proximity</code>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "housing_num = housing.drop('ocean_proximity', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can fit the imputer to the training data using the <code>fit()</code> method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = imputer.fit(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The imputer has simply computed the median of each attribute and stored the result in its <code>statistics_</code> instance variable. Only the <code>total_bedrooms</code> attribute had missing values, but we cannot be sure there won't be any missing values in new data after the system goes live, so it's safer to apply the imputer to all the numerical attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -118.51  ,    34.26  ,    29.    ,  2119.5   ,   433.    ,\n",
       "        1164.    ,   408.    ,     3.5409])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -118.51  ,    34.26  ,    29.    ,  2119.5   ,   433.    ,\n",
       "        1164.    ,   408.    ,     3.5409])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_num.median().values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use this \"trained\" imputer to transform the training set by replacing missing values by the learned medians:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = imputer.transform(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is plain Numpy array containing the transformed features. If you want to put it back in a Pandas DataFrame, it's simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For details of sklearn's API design principles see http://goo.gl/wL10sI\n",
    "\n",
    "Short version:\n",
    "- <b>Estimators:</b> For example an Imputer. Computes some values through the <code>fit()</code> method.\n",
    "- <b>Transformers:</b> Transform a dataset through the <code>transform()</code> method and returns the transformed dataset. Generally reies on a Estimator's <code>fit()</code> estimation.\n",
    "- For convience there is also generally a <code>fit_transform()</code> method to a Transformer whch could run much faster sometimes.\n",
    "- <b>Predictors:</b> Some Estimators are capable of making a prediction given a dataset. For example LinearRegression model. A predictor has a <code>predict()</code> and a <code>score()</code> method which measures the accuracy of the prediction given a test set.\n",
    "\n",
    "\n",
    "## Handling Text and Catagorical Attributes\n",
    "\n",
    "We removed the categorical attribute <code>ocean_proximity</code> earlier because it is a text attribute so we cannot compute it's median. Most Machine Learning algos prefer to work with numbers anyway so let's conert these text labels to numbers.\n",
    "\n",
    "Scikit-Learn provides a transformer for this task called <code>LabelEncoder</code>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 4, ..., 1, 0, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "housing_cat = housing['ocean_proximity']\n",
    "housing_cat_encoded = encoder.fit_transform(housing_cat)\n",
    "housing_cat_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this numerical data in any ML algo. You can look at the mapping tht this encode has learned using the <code>classes_</code> attribute (the index into the list gives the mapping):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<1H OCEAN' 'INLAND' 'ISLAND' 'NEAR BAY' 'NEAR OCEAN']\n"
     ]
    }
   ],
   "source": [
    "print(encoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An issue with this representation is that many ML algos will assume that two nearby values are more similar than two distant values, which is not the case. Use <i>one-hot encoding</i> to fix this. That creates one binary attribute per category with a 1 when the category is matched for that data row. \n",
    "\n",
    "Scikit-Learn provides a <code>OneHotEncoder</code> encoder to convert integer categorical values into one-hot encoded vectors. Let's do that... Note that <code>fit_transform()</code> expects a 2D array, but <code>housing_cat_encoded</code> is a 1D array so we need to reshape it (see SciPy's documentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<16512x5 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 16512 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "housing_cat_1hot = encoder.fit_transform(housing_cat_encoded.reshape(-1, 1))\n",
    "housing_cat_1hot"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Notice that the output is a SciPy <i>sparse matrix</i> instead of a NumPy array. This is very useful when you have categorical attributes with thousands of categories. After one-hot encoding we get a matrix with thousands of columns and the matrix is full of zeros except for one 1 per row. Using up tons of memory mostly to store zeros would be very wasteful so instead a sparce matrix only stores the location of the nonzero elements. You can use it like a normal 2D array but if you really want to convert it to a (dense) NumPy array, do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.],\n",
       "       ..., \n",
       "       [ 0.,  1.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_cat_1hot.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply both steps (from text to integer categories and then to one-hot encodings) using the <code>LabelBinarizer</code> class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       ..., \n",
       "       [0, 1, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "encoder = LabelBinarizer()\n",
    "housing_cat_1hot = encoder.fit_transform(housing_cat)\n",
    "housing_cat_1hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this returns a dense NumPy matrix by default but you can get a sparce matrix by passing <code>sparse_output=True</code> to the <code>LabelBinarizer</code> constructor.\n",
    "\n",
    "## Custom Transformers\n",
    "\n",
    "You will need to write your own transformers for tasks such as custom cleanup operations or combining specific attributes. You will want these to be compliant with Scikit-Learn's API so they work seamlessly with things like sklearn pipelines. Since sklearn relies on duck typing, you only need to create a class that implements 3 methods: <code>fit()</code> (returning <code>self</code>), <code>transform()</code> and <code>fit_transform()</code>. You can get the last one for free by adding <code>TransformMixin</code> as a base class. Also, if you add <code>BaseEstimator</code> as a base class (and avoid <code>*args</code> and <code>**kargs</code> in your constructor) you will get two extra methods: <code>get_params()</code> and <code>set_parms()</code> that will be useful for automatic hyperparameter tuning. Example of a small transformer class that adds the combined attributes we discussed earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "rooms_ix, bedrooms_ix, population_ix, household_ix = 3, 4, 5, 6\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room = True):        # avoid *args and **kargs\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self      # nothin else to do \n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, household_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, household_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n",
    "\n",
    "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\n",
    "housing_extra_attribs = attr_adder.transform(housing.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example the transformer had one hyperparamter, <code>add_bedrooms_per_room</code> that was set to True by default. This hyperparamter will allow you to easily test whether or not adding it makes a difference to the algorithm. More generally you can use hyperparameters like this to gate any data preparation step that you are not sure about. The more you automate these data preparation steps the more combinations you can automatically try out, making it more likely you will find a great combination and saving you a lot of time. \n",
    "\n",
    "## Feature Scaling\n",
    "\n",
    "One of the most important transformations you need to apply is <i>feature scaling</i>. With few exceptions ML algos don't perform well when the input numerical attributes have very different scales. For example the total number of rooms ranges from 6 to 39,320 while the median incomes only range from 0 to 15. Note that scaling the target values is generally not required. \n",
    "\n",
    "There are two common ways to get all attributes to have the same scale: <b><i>min-max scaling</i></b> and <b><i>standardization</i></b>.\n",
    "\n",
    "Min-max scaling works by squashing all values into a range between 0 and 1. We do this by subtracting the minimum and then dividing by the difference between the max and the min. Scikit-Learn provides a transformer called <code>MinMaxScaler</code> for this. It has a <code>feature_range</code> hyperparameter that lets you change the range if you don't want 0-1 for any reason. \n",
    "\n",
    "Standardization subtracts the mean (so standardized values always have the same mean) and the divides by the variance so that the resulting distribution has unit variance. This does not bound the values to a specific range, unlike min-max scaling, which may be a problem for some algorithms (e.g. neural networks often expect an input value to range from 0 to 1). But standardization is much less affected by outliers. For example, suppose a district had a median income of 100 (by mistake). Then min-max scaling would squash everything in the range 0 to 15 down to 0-0.15, whereas standardization would not be much affected. Scikit-Learn provides a transformer called <code>StandardScaler</code> for standardization. \n",
    "\n",
    "<font color=\"#850000\">\n",
    "As with all the transformations, only fit the scalers to the training data, not the full dataset (including the test set). Only then can you use them to transform the training set and the test set (and new data).\n",
    "</font><br>\n",
    "\n",
    "## Transformation Pipelines\n",
    "\n",
    "There are many data transformation steps that need to be executed in the right order. Scikit-Learn provides the <code>Pipeline</code> class to help with such sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', Imputer(strategy='median')),\n",
    "    ('attribs_adder', CombinedAttributesAdder()),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All but the last name/estimator pair in the Pipeline constructor must be transformers (i.e. they must have a <code>fit_transform()</code> method).\n",
    "\n",
    "When you call the pipepline's fit() method it calls fit_transform() sequentially on all transforms, passing the output from each one as the input to the next. When the final transformer is reached it only calls it's fit() method. \n",
    "\n",
    "The pipeline exposes the same methods as the final estimator. \n",
    "\n",
    "You have a pipeline for the numerical values and you still need to apply the <code>LabelBinarizer</code> on the categorical values. Scikit-Learn provides a <code>FeatureUnion</code> class to join these transformations into a single pipeline. You give it a list of transformers (which can be entire pipelines) and when it's transform() method is called it runs each transformers transform() method in parallel, waits for their output and then concatenates them and returns the result. Here's a full pipeline handling both numerical and categorical attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.15604281,  0.77194962,  0.74333089, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-1.17602483,  0.6596948 , -1.1653172 , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 1.18684903, -1.34218285,  0.18664186, ...,  0.        ,\n",
       "         0.        ,  1.        ],\n",
       "       ..., \n",
       "       [ 1.58648943, -0.72478134, -1.56295222, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.78221312, -0.85106801,  0.18664186, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-1.43579109,  0.99645926,  1.85670895, ...,  0.        ,\n",
       "         1.        ,  0.        ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "# Refresh to get clean data again\n",
    "housing, housing_labels = load_data()\n",
    "housing_num = housing.drop('ocean_proximity', axis=1)\n",
    "\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].values\n",
    "\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = ['ocean_proximity']\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('selector', DataFrameSelector(num_attribs)),\n",
    "    ('imputer', Imputer(strategy='median')),\n",
    "    ('attribs_adder', CombinedAttributesAdder()),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('selector', DataFrameSelector(cat_attribs)),\n",
    "    ('label_binarizer', LabelBinarizer()),\n",
    "])\n",
    "\n",
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    "    ('num_pipeline', num_pipeline),\n",
    "    ('cat_pipeline', cat_pipeline),\n",
    "])\n",
    "\n",
    "housing_prepared = full_pipeline.fit_transform(housing)\n",
    "housing_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16512, 16)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_prepared.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is nothing in Scikit-Learn to handle Pandas DataFrames so the DataFrameSelector class accomplishes this task, as well as selecting the relevant columns for each pipeline. (but see Pull Request #3886 in sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________________________________________________________________________________\n",
    "\n",
    "<b>That's it for this part. Now pickle the data and make it ready for import in part 3.</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle, os\n",
    "\n",
    "if not os.path.isdir('./data'):\n",
    "    os.makedirs('./data')\n",
    "\n",
    "with open('./data/prt2.pkle', 'wb') as f:\n",
    "    pickle.dump({'housing': housing, \n",
    "                 'housing_prepared': housing_prepared, \n",
    "                 'housing_labels': housing_labels,\n",
    "                 'full_pipeline': full_pipeline,\n",
    "                 'encoder': encoder,\n",
    "                 'num_attribs': num_attribs,\n",
    "                }, f, pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
